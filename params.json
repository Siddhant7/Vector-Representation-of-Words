{
  "name": "word2vec",
  "tagline": "",
  "body": "In this article, I try to explain the very popular word2vec model by [Tomas Mikolov](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). This model is used for learning vector representations of words, called \"word embeddings\".(supervised vs unsupervised)\r\n\r\n### **Highlights**\r\n\r\n* I start by giving the motivation for why we would want to represent words as vectors.\r\n* I will then briefly cover on the traditional methods used for word embeddings.\r\n* After introducing the concept of probabilistic language modelling, we will take a deep dive into our word2vec model(which is nothing, but a neural probabilistic language model without non-linearity).\r\n* In the end, I will discuss about the practical issues of the model, and how to deal with it.\r\n\r\n### **Motivation: Why Learn Word Embeddings?**\r\n\r\nIn order for a computer to process natural language, we need to create representation for the language, often times, the representation for the words. In many NLP tasks that use mathematical models such as neural networks, we would want to give an input to these models which should be in some mathematical format. We cannot give just raw words as input to the network. \r\n\r\n### **Traditional Methods** \r\n\r\n**Bag of Words**\r\n\r\nEnglish language has in the order of 100,000 words. One can represent each word as a one-hot vector of dimension 100,000(e.g. the word \"cat\" will be represented as V-dimensional vector : (0,0,0,1,0,0....0) if it's index is 4 in our dictionary). This is a sparse and high dimension input. Each word in the vocabulary is represented by one bit position in a HUGE vector. Moreover, contextual information is not utilized by this approach. It will just act like a look-up table.\r\n\r\nOur goal is to map this into dense, continuous and low dimensional input of around 300 dimension(e.g. the word \"cat\" now will be represented as a 300-dimensional vector which may look like (-0.4, 0.3, 0,6,.....)), such that the **semantically similar** words are mapped close to each other. This is where Vector Space Models(VSMs) come into picture. VSMs represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other'). VSMs have a long, rich history in NLP, but all methods depend in some way or another on the Distributional Hypothesis, which states that words that appear in the same contexts share semantic meaning. The different approaches that leverage this principle can be divided into two categories: count-based methods (e.g. Latent Semantic Analysis), and predictive methods (e.g. neural probabilistic language models).\r\n\r\n**Matrix-Factorization**\r\n\r\nMatrix factorization methods are count-based methods and are very popular for generating low-dimensional word representations. These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. The particular type of information captured by such matrices varies by application. In LSA, the matrices are of “term-document” type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus. The entries in the matrix are filled according to the frequency of each term in each document. The consequence of the rank lowering is that some dimensions are combined and depend on more than one term: \r\n\r\n{(car), (truck), (flower)} --> {(1.3452 * car + 0.2828 * truck), (flower)}\r\n\r\nNow, what do we mean by \"low-rank approximation\" of a matrix? it simply means combining similar row vectors into one, as in the above example. This is achieved by a technique known as Singular Value Decomposition(SVD).\r\n\r\n### **Language Modelling**\r\n\r\nWord embedding models are quite closely intertwined with language models. The quality of language models is measured based on their ability to learn a probability distribution over words in V. Before we get into the gritty details of word2vec model, let us briefly talk about some language modelling fundamentals.\r\nLanguage models generally try to compute the probability of a word wt given its n previous words, i.e. p(wt|wt−1,⋯wt−n+1).\r\n\r\nBy applying the chain rule together with the Markov assumption, we can approximate the product of a whole sentence or document by the product of the probabilities of each word given its n previous words: \r\n\r\np(w1,⋯,wT)=∏ip(wi|wi−1,⋯,wi−n+1)p(w1,⋯,wT)=∏ip(wi|wi−1,⋯,wi−n+1).\r\n\r\nIn n-gram based language models, we can calculate a word's probability based on the frequencies of its constituent n-grams:\r\n\r\np(wt|wt−1,⋯,wt−n+1)=count(wt−n+1,⋯,wt−1,wt)/count(wt−n+1,⋯,wt−1)\r\n\r\nIf n=0, we call it unigram, if n=1, we call it bigram, and so on. We can extend it to trigram, 4-gram, 5-gram. But in general, we keep the value of n low, because as n increases, the sentence length that we need to find in the data increases. So, the occurrence of these long length sentences will be usually less in our corpus.\r\nIn general, n-gram is an inefficient model of language. This is because language has long-distance dependencies. For example, consider the sentence :\r\n\r\n_\"The computer which I had just put into the machine room on the fifth floor crashed\"._ \r\n\r\nWe would get poor prediction if we tried to predict the word \"crashed\" by considering even the 5 previous words(6-gram model).\r\n\r\n### Word2vec Model\r\n\r\nWord2vec is one of the most popular model for representing words as vectors. It is often referred to as unsupervised learning(though we use NN) only in the sense that it does not require labelled data. It uses the concept of neural language modelling to learn vector representation of words. In neural language models, we achieve the same objective of predicting the next word of a sentence by using the well-known softmax function\r\n\r\n![Softmax](https://www.google.co.in/search?q=softmax+function&client=ubuntu&hs=AP1&channel=fs&source=lnms&tbm=isch&sa=X&ved=0ahUKEwi5jN2mvdvQAhVHLI8KHT8OBuUQ_AUICCgB&biw=1301&bih=673#channel=fs&tbm=isch&q=softmax+function+word2vec&imgrc=PAsu5C03qSeX_M%3A)\r\n\r\np(wt|wt−1,⋯,wt−n+1)=exp(h⊤v′wt)/∑wi∈Vexp(h⊤v′wi)\r\n\r\nThe word2vec model can be trained via 2 tasks :\r\n* Continuous Bag of Words(CBOW) : Predicts a word with input as a sequence of multiple words. This sequence is referred to as the context of the word that needs to be predicted.\r\n* Skip Gram(SG) : Predicts multiple words with input as a context word. In this, a single word acts as a context to the multiple words being predicted.\r\n\r\nI will discuss the CBOW model in this article, but the idea remains the same for SG model as well. Theoretically, word2vec is a simple feed forward neural network, with a single hidden layer. We give the input to the network as the sum of all one-hot vectors of the corresponding context words. Let this final sum of vectors be represented by X. So X will be a Vx1 matrix, where V is the size of our dictionary. So X will somewhat look like this : (0,0,1,0,1,0,0,1,0....0), where 1 will occupy in the index of the context word in our dictionary, and 0 in the rest. Now we give this vector X as the input to our neural network.\r\n\r\n![Word2vec Architecture](http://postachio-images.s3-website-us-east-1.amazonaws.com/d7832041-1d7e-4ec7-bb7e-099193cc8c7d/097950f1-2231-48cb-9c3a-dcefa8ea3001/f297b956-90f2-4c29-9208-12be0050e819.png)",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}