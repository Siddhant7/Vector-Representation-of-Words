<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Vector Representation of Words by Siddhant7</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Vector Representation of Words</h1>
        <h2></h2>
        <a href="https://github.com/Siddhant7/Vector-Representation-of-Words" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <p>In this article, I try to explain the very popular word2vec model by <a href="https://arxiv.org/pdf/1301.3781.pdf">Tomas Mikolov</a>. This model is used for learning vector representations of words, called "word embeddings".</p>

<h3>
<a id="highlights" class="anchor" href="#highlights" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Highlights</strong>
</h3>

<ul>
<li>I start by giving the motivation for why we would want to represent words as vectors.</li>
<li>I will then briefly cover on the traditional methods used for word embeddings.</li>
<li>After introducing the concept of probabilistic language modelling, we will take a deep dive into our word2vec model(which is nothing, but a neural probabilistic language model without non-linearity).</li>
<li>In the end, I will discuss about the practical issues of the model, and how to deal with it.</li>
</ul>

<h3>
<a id="motivation-why-learn-word-embeddings" class="anchor" href="#motivation-why-learn-word-embeddings" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Motivation: Why Learn Word Embeddings?</strong>
</h3>

<p>In order for a computer to process natural language, we need to create representation for the language, often times, the representation for the words. In many NLP tasks that use mathematical models such as neural networks, we would want to give an input to these models which should be in some mathematical format. We cannot give just raw words as input to the network. </p>

<h3>
<a id="traditional-methods" class="anchor" href="#traditional-methods" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Traditional Methods</strong>
</h3>

<p><strong>Bag of Words</strong></p>

<p>English language has in the order of 100,000 words. One can represent each word as a one-hot vector of dimension 100,000(e.g. the word "cat" will be represented as V-dimensional vector : (0,0,0,1,0,0....0) if it's index is 4 in our dictionary). This is a sparse and high dimension input. Each word in the vocabulary is represented by one bit position in a HUGE vector. Moreover, contextual information is not utilized by this approach. It will just act like a look-up table.</p>

<p>Our goal is to map this into dense, continuous and low dimensional input of around 300 dimension(e.g. the word "cat" now will be represented as a 300-dimensional vector which may look like (-0.4, 0.3, 0,6,.....)), such that the <strong>semantically similar</strong> words are mapped close to each other. This is where Vector Space Models(VSMs) come into picture. VSMs represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other'). VSMs have a long, rich history in NLP, but all methods depend in some way or another on the Distributional Hypothesis, which states that words that appear in the same contexts share semantic meaning. The different approaches that leverage this principle can be divided into two categories: count-based methods (e.g. Latent Semantic Analysis), and predictive methods (e.g. neural probabilistic language models).</p>

<p><strong>Matrix-Factorization</strong></p>

<p>Matrix factorization methods are count-based methods and are very popular for generating low-dimensional word representations. These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus. The particular type of information captured by such matrices varies by application. In LSA, the matrices are of “term-document” type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus. The entries in the matrix are filled according to the frequency of each term in each document. The consequence of the rank lowering is that some dimensions are combined and depend on more than one term: </p>

<p>{(car), (truck), (flower)} --&gt; {(1.3452 * car + 0.2828 * truck), (flower)}</p>

<p>Now, what do we mean by "low-rank approximation" of a matrix? it simply means combining similar row vectors into one, as in the above example. This is achieved by a technique known as Singular Value Decomposition(SVD).</p>

<h3>
<a id="language-modelling" class="anchor" href="#language-modelling" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Language Modelling</strong>
</h3>

<p>Word embedding models are quite closely intertwined with language models. The quality of language models is measured based on their ability to learn a probability distribution over words in V. Before we get into the gritty details of word2vec model, let us briefly talk about some language modelling fundamentals.
Language models generally try to compute the probability of a word wt given its n previous words, i.e. p(wt|wt−1,⋯wt−n+1).</p>

<p>By applying the chain rule together with the Markov assumption, we can approximate the product of a whole sentence or document by the product of the probabilities of each word given its n previous words: </p>

<p>p(w1,⋯,wT)=∏ip(wi|wi−1,⋯,wi−n+1)p(w1,⋯,wT)=∏ip(wi|wi−1,⋯,wi−n+1).</p>

<p>In n-gram based language models, we can calculate a word's probability based on the frequencies of its constituent n-grams:</p>

<p>p(wt|wt−1,⋯,wt−n+1)=count(wt−n+1,⋯,wt−1,wt)/count(wt−n+1,⋯,wt−1)</p>

<p>If n=0, we call it unigram, if n=1, we call it bigram, and so on. We can extend it to trigram, 4-gram, 5-gram. But in general, we keep the value of n low, because as n increases, the sentence length that we need to find in the data increases. So, the occurrence of these long length sentences will be usually less in our corpus.
In general, n-gram is an inefficient model of language. This is because language has long-distance dependencies. For example, consider the sentence :</p>

<p><em>"The computer which I had just put into the machine room on the fifth floor crashed".</em> </p>

<p>We would get poor prediction if we tried to predict the word "crashed" by considering even the 5 previous words(6-gram model).</p>

<h3>
<a id="word2vec-model" class="anchor" href="#word2vec-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Word2vec Model</h3>

<p>Word2vec is one of the most popular model for representing words as vectors. It is often referred to as unsupervised learning(though we use NN) only in the sense that it does not require labelled data. It uses the concept of neural language modelling to learn vector representation of words. In neural language models, we achieve the same objective of predicting the next word of a sentence by using the well-known softmax function.</p>

<p>The word2vec model can be trained via 2 tasks :</p>

<ul>
<li>Continuous Bag of Words(CBOW) : Predicts center word from sum of surrounding word vectors. These surrounding word vectors are referred to as the context of the center word.</li>
<li>Skip Gram(SG) : Predicts surrounding words by taking center word as the input. In this, the center word acts as a context to the surrounding words being predicted.</li>
</ul>

<p>We will go through the CBOW model in this article, but the idea remains the same for SG model as well. Theoretically, word2vec is a <strong>fully connected feed forward neural network</strong>, with a single hidden layer. Below image describes the model architecture.</p>

<p><img src="http://postachio-images.s3-website-us-east-1.amazonaws.com/d7832041-1d7e-4ec7-bb7e-099193cc8c7d/097950f1-2231-48cb-9c3a-dcefa8ea3001/f297b956-90f2-4c29-9208-12be0050e819.png" alt="Word2vec Architecture"></p>

<p><strong>Input Layer</strong>
We give the input to the network as the sum of all one-hot vectors of the corresponding context words. Let this final sum of vectors be represented by X. So X will be a Vx1 matrix, where V is the size of our dictionary. So X will somewhat look like this : (0,0,1,0,1,0,0,1,0....0), where 1 will occupy in the index of the context word in our dictionary, and 0 in the rest. Let us take input as a single word, for simplicity. Now we give this vector X as the input to our neural network.</p>

<p><strong>Hidden Layer</strong>
The input vector is then multiplied by the transpose of W, which is a VxN matrix. As the input vector is of the form (0,0,0,1,0,0,0...0), we just need to find one row vector of W(4th row in this case) instead of the whole matrix. The result is an Nx1 vector which is represented by h.</p>

<p><strong>Output Layer</strong>
This vector h is further multiplied by the transpose of W', which is an NxV matrix. The result would be a Vx1 vector. But we already know that the output vector is something of the form (0,1,0,0,0...0) where 1 would occupy the index of the next word in the sentence, which we are trying to predict. So by comparing the matrix multiplication with the actual output, we can observe that we just need to find one column vector of W'(2nd in this case) instead of the whole matrix.</p>

<p>In order to get probabilities, we apply softmax function to this output.</p>

<p><img src="http://i.stack.imgur.com/m4x7n.jpg" alt="Softmax"></p>

<p>So we have to find a row vector of W(the second term in the numerator) and a column vector of W'(the first term in the numerator). In fact, these 2 vectors are the vector representations of the output word! Yes, each word is represented by two vectors, which can finally be summed to get one single representation. In order to find these two vectors, we first need to define our objective function.
As we are dealing with probabilities, we would choose the maximum likelihood as the objective function to maximize this probability. Cost function J = -summation[log(P)], where P is as defined above. Now by applying Gradient Descent on this softmax probability together with the help of backpropagation, one can find these two vectors.</p>

<p><img src="http://image.slidesharecdn.com/datadaytalk-160116232525/95/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-37-638.jpg?cb=1452987138" alt="GD"></p>

<p>Notice that there is a summation in the denominator of the softmax probability. The derivative of the softmax will also consist of this summation. So in every iteration of gradient descent, one would have to calculate this sum, which will take a lot of time, specially if the size of our vocabulary is large, which is often the case. So we cannot use this in practice. But Tomas Mikolov also published <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">this</a> paper in which he has introduced different objective functions to get away with the practical issues of the model. I will explain the one which is most successful and widely used : Negative Sampling.</p>

<h3>
<a id="negative-sampling" class="anchor" href="#negative-sampling" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Negative Sampling</h3>

<p>To get rid of the normalization factor in softmax, Mikolov introduced Negative Sampling. In this, we treat the problem completely differently. We will assume that we are solving a classification problem. Consider two sets S and S'. S will contain all the (w,c) pairs that occur together in our corpus, and S' will contain the pairs which do not occur together. S' can be formed by random selection of words from the corpus. So we will give a pair to our classifier, and the classifier should assign a label to it, 1 means it is in the corpus, 0 means it is not in the corpus.
Pr(D=1 | (w,c)) is the probability that a pair is in S.
Pr(D=0 | (w,c)) is the probability that a pair is not in S.
Now we solve this classification problem using logistic regression. Probability distribution of D can be defined as</p>

<p>[Pr(D=1 | (w,c))]^D * [1-Pr(D=1 | (w,c))]^1-D</p>

<p>We use maximum likelihood to maximize this probability.</p>

<p><img src="http://images2015.cnblogs.com/blog/754644/201607/754644-20160729151919169-1347568501.jpg" alt="NS"></p>

<p>Here _V_c is the row vector of W and _V_w is the column vector of W', same as it was in the numerator of the softmax function. These 2 vectors can now be calculated by using gradient descent, but this time the derivative required to calculate the updated vectors does not contain the summation as it was in the softmax function. </p>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/Siddhant7/Vector-Representation-of-Words/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/Siddhant7/Vector-Representation-of-Words/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/Siddhant7/Vector-Representation-of-Words"></a> is maintained by <a href="https://github.com/Siddhant7">Siddhant7</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
